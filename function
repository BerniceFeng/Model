import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, fbeta_score

def metrics_table(y_pred, y_test, label_names):
    metrics = []
    n_labels = len(label_names)
    fig, axes = plt.subplots(5, 3, figsize=(15, 20))
    axes = axes.ravel()  # Flatten the 5x3 array of axes
    
    for i in range(y_test.shape[1]):
        precision = precision_score(y_test[:, i], y_pred[:, i])
        recall = recall_score(y_test[:, i], y_pred[:, i])
        f1 = f1_score(y_test[:, i], y_pred[:, i])
        f0_5 = fbeta_score(y_test[:, i], y_pred[:, i], beta=0.5)
        support = y_test[:, i].sum()
        
        # Calculate precision-recall curve and AUC
        precision_curve, recall_curve, _ = precision_recall_curve(y_test[:, i], y_pred[:, i])
        pr_auc = auc(recall_curve, precision_curve)
        
        # Plot the PR curve in the corresponding subplot
        axes[i].plot(recall_curve, precision_curve, lw=2, label=f'AUC = {pr_auc:.2f}')
        axes[i].set_xlabel('Recall')
        axes[i].set_ylabel('Precision')
        axes[i].set_title(f'{label_names[i]} PR Curve')
        axes[i].legend(loc='lower left')
        axes[i].grid(True)
        
        metrics.append({
            'Label': label_names[i],
            'Precision': f'{precision * 100:.2f}%',
            'Recall': f'{recall * 100:.2f}%',
            'F1 Score': f'{f1 * 100:.2f}%',
            'F0.5 Score': f'{f0_5 * 100:.2f}%',
            'Support': f'{support:.0f}',
            'PR AUC': f'{pr_auc:.2f}'
        })
    
    label_pred = y_pred
    label_true = y_test
    
    # Calculate and plot micro-average PR curve
    micro_precision, micro_recall, _ = precision_recall_curve(label_true.ravel(), label_pred.ravel())
    micro_auc = auc(micro_recall, micro_precision)
    axes[n_labels].plot(micro_recall, micro_precision, lw=2, label=f'Micro Average AUC = {micro_auc:.2f}')
    axes[n_labels].set_xlabel('Recall')
    axes[n_labels].set_ylabel('Precision')
    axes[n_labels].set_title('Micro Average PR Curve')
    axes[n_labels].legend(loc='lower left')
    axes[n_labels].grid(True)
    
    metrics.append({
        'Label': 'Micro Average',
        'Precision': f'{micro_precision.mean() * 100:.2f}%',
        'Recall': f'{micro_recall.mean() * 100:.2f}%',
        'F1 Score': f'{f1_score(label_true, label_pred, average="micro") * 100:.2f}%',
        'F0.5 Score': f'{fbeta_score(label_true, label_pred, average="micro", beta=0.5) * 100:.2f}%',
        'Support': f'{label_true.sum():.0f}',
        'PR AUC': f'{micro_auc:.2f}'
    })
    
    # Calculate and plot macro-average PR curve
    macro_precision = []
    macro_recall = []
    for i in range(y_test.shape[1]):
        precision_curve, recall_curve, _ = precision_recall_curve(y_test[:, i], y_pred[:, i])
        macro_precision.append(precision_curve)
        macro_recall.append(recall_curve)
    
    # Interpolating all curves to the same recall points and averaging
    all_recalls = np.unique(np.concatenate(macro_recall))
    mean_precision = np.zeros_like(all_recalls)
    for recall_curve, precision_curve in zip(macro_recall, macro_precision):
        mean_precision += np.interp(all_recalls, recall_curve, precision_curve)
    mean_precision /= len(macro_precision)
    macro_auc = auc(all_recalls, mean_precision)
    
    axes[n_labels + 1].plot(all_recalls, mean_precision, lw=2, label=f'Macro Average AUC = {macro_auc:.2f}')
    axes[n_labels + 1].set_xlabel('Recall')
    axes[n_labels + 1].set_ylabel('Precision')
    axes[n_labels + 1].set_title('Macro Average PR Curve')
    axes[n_labels + 1].legend(loc='lower left')
    axes[n_labels + 1].grid(True)
    
    metrics.append({
        'Label': 'Macro Average',
        'Precision': f'{precision_score(label_true, label_pred, average="macro") * 100:.2f}%',
        'Recall': f'{recall_score(label_true, label_pred, average="macro") * 100:.2f}%',
        'F1 Score': f'{f1_score(label_true, label_pred, average="macro") * 100:.2f}%',
        'F0.5 Score': f'{fbeta_score(label_true, label_pred, average="macro", beta=0.5) * 100:.2f}%',
        'Support': f'{label_true.sum():.0f}',
        'PR AUC': f'{macro_auc:.2f}'
    })
    
    results_df = pd.DataFrame(metrics)
    
    # Hide any remaining empty subplots
    for j in range(n_labels + 2, len(axes)):
        fig.delaxes(axes[j])
    
    plt.tight_layout()
    plt.show()  # Ensure the plot is displayed
    
    return results_df
