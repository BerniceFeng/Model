import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score, fbeta_score

def metrics_table(y_pred, y_test, label_names):
    metrics = []
    n_labels = len(label_names)
    fig, axes = plt.subplots(5, 3, figsize=(15, 20))
    axes = axes.ravel()  # Flatten the 5x3 array of axes
    
    for i in range(y_test.shape[1]):
        precision = precision_score(y_test[:, i], y_pred[:, i])
        recall = recall_score(y_test[:, i], y_pred[:, i])
        f1 = f1_score(y_test[:, i], y_pred[:, i])
        f0_5 = fbeta_score(y_test[:, i], y_pred[:, i], beta=0.5)
        support = y_test[:, i].sum()
        
        # Calculate precision-recall curve and AUC
        precision_curve, recall_curve, _ = precision_recall_curve(y_test[:, i], y_pred[:, i])
        pr_auc = auc(recall_curve, precision_curve)
        
        # Plot the PR curve in the corresponding subplot
        axes[i].plot(recall_curve, precision_curve, lw=2, label=f'AUC = {pr_auc:.2f}')
        axes[i].set_xlabel('Recall')
        axes[i].set_ylabel('Precision')
        axes[i].set_title(f'{label_names[i]} PR Curve')
        axes[i].legend(loc='lower left')
        axes[i].grid(True)
        
        metrics.append({
            'Label': label_names[i],
            'Precision': f'{precision * 100:.2f}%',
            'Recall': f'{recall * 100:.2f}%',
            'F1 Score': f'{f1 * 100:.2f}%',
            'F0.5 Score': f'{f0_5 * 100:.2f}%',
            'Support': f'{support:.0f}',
            'PR AUC': f'{pr_auc:.2f}'
        })
    
    label_pred = y_pred
    label_true = y_test
    
    # Calculate and plot micro-average PR curve
    micro_precision, micro_recall, _ = precision_recall_curve(label_true.ravel(), label_pred.ravel())
    micro_auc = auc(micro_recall, micro_precision)
    axes[n_labels].plot(micro_recall, micro_precision, lw=2, label=f'Micro Average AUC = {micro_auc:.2f}')
    axes[n_labels].set_xlabel('Recall')
    axes[n_labels].set_ylabel('Precision')
    axes[n_labels].set_title('Micro Average PR Curve')
    axes[n_labels].legend(loc='lower left')
    axes[n_labels].grid(True)
    
    metrics.append({
        'Label': 'Micro Average',
        'Precision': f'{micro_precision.mean() * 100:.2f}%',
        'Recall': f'{micro_recall.mean() * 100:.2f}%',
        'F1 Score': f'{f1_score(label_true, label_pred, average="micro") * 100:.2f}%',
        'F0.5 Score': f'{fbeta_score(label_true, label_pred, average="micro", beta=0.5) * 100:.2f}%',
        'Support': f'{label_true.sum():.0f}',
        'PR AUC': f'{micro_auc:.2f}'
    })
    
    # Calculate and plot macro-average PR curve
    macro_precision, macro_recall, _ = precision_recall_curve(label_true.ravel(), label_pred.ravel(), average='macro')
    macro_auc = auc(macro_recall, macro_precision)
    axes[n_labels + 1].plot(macro_recall, macro_precision, lw=2, label=f'Macro Average AUC = {macro_auc:.2f}')
    axes[n_labels + 1].set_xlabel('Recall')
    axes[n_labels + 1].set_ylabel('Precision')
    axes[n_labels + 1].set_title('Macro Average PR Curve')
    axes[n_labels + 1].legend(loc='lower left')
    axes[n_labels + 1].grid(True)
    
    metrics.append({
        'Label': 'Macro Average',
        'Precision': f'{precision_score(label_true, label_pred, average="macro") * 100:.2f}%',
        'Recall': f'{recall_score(label_true, label_pred, average="macro") * 100:.2f}%',
        'F1 Score': f'{f1_score(label_true, label_pred, average="macro") * 100:.2f}%',
        'F0.5 Score': f'{fbeta_score(label_true, label_pred, average="macro", beta=0.5) * 100:.2f}%',
        'Support': f'{label_true.sum():.0f}',
        'PR AUC': f'{macro_auc:.2f}'
    })
    
    results_df = pd.DataFrame(metrics)
    
    # Hide any remaining empty subplots
    for j in range(n_labels + 2, len(axes)):
        fig.delaxes(axes[j])
    
    plt.tight_layout()
    plt.show()  # Ensure the plot is displayed
    
    return results_df





class ConvNet(nn.Module):
    def __init__(self, output_size, hidden_size, dropout_rate):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, hidden_size, kernel_size=(3, 1))
        self.bn1 = nn.BatchNorm2d(hidden_size)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.conv2 = nn.Conv2d(hidden_size, hidden_size, kernel_size=(3, 1))
        self.bn2 = nn.BatchNorm2d(hidden_size)
        self.dropout2 = nn.Dropout(dropout_rate)
        self.fc_input_dim = self._get_conv_output_dim()
        self.fc = nn.Linear(self.fc_input_dim, output_size)

    def _get_conv_output_dim(self):
        with torch.no_grad():
            dummy_input = torch.zeros(1, 1, n_features, 1)
            x = self.conv1(dummy_input)
            x = self.conv2(x)
            x = x.view(1, -1)
            return x.size(1)

    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.dropout1(x)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.dropout2(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = torch.sigmoid(self.fc(x))  # For multilabel classification
        return x
